{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e538515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ultralytics.models.yolo.detect import DetectionTrainer\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe02f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Custom Dataset that streams from URLs\n",
    "class YOLOStreamingDataset(Dataset):\n",
    "    def __init__(self, data_list, imgsz=640):\n",
    "        \"\"\"\n",
    "        data_list: List of dicts: \n",
    "        {'url': '...', 'bboxes': [[x,y,w,h], ...], 'classes': [0, ...]}\n",
    "        \"\"\"\n",
    "        self.data = data_list\n",
    "        self.imgsz = imgsz\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Download image\n",
    "        try:\n",
    "            response = requests.get(item['url'], timeout=10)\n",
    "            img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {item['url']}: {e}\")\n",
    "            # Return a blank image as a fallback (or handle skip logic)\n",
    "            img = Image.new('RGB', (self.imgsz, self.imgsz), (0, 0, 0))\n",
    "\n",
    "        w0, h0 = img.size\n",
    "        img = np.array(img)\n",
    "        \n",
    "        # Resize image for the model\n",
    "        img = cv2.resize(img, (self.imgsz, self.imgsz))\n",
    "        img = img.transpose(2, 0, 1)  # HWC to CHW\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        # Convert COCO [x,y,w,h] to YOLO [cls, cx, cy, w, h] normalized\n",
    "        labels = []\n",
    "        for i, bbox in enumerate(item['bboxes']):\n",
    "            x, y, w, h = bbox\n",
    "            cls = item['classes'][i]\n",
    "            # Normalize to [0, 1]\n",
    "            cx = (x + w / 2) / w0\n",
    "            cy = (y + h / 2) / h0\n",
    "            wn = w / w0\n",
    "            hn = h / h0\n",
    "            labels.append([cls, cx, cy, wn, hn])\n",
    "        \n",
    "        # Capture original shape before resizing\n",
    "        # YOLO expects (height, width)\n",
    "        ori_shape = (h0, w0) \n",
    "        \n",
    "        # We are stretching the image to imgsz, so ratio is (new/old)\n",
    "        # and padding is zero because we aren't letterboxing\n",
    "        ratio_pad = ((self.imgsz / h0, self.imgsz / w0), (0, 0))\n",
    "\n",
    "        return {\n",
    "            'img': torch.from_numpy(img).float() / 255.0,\n",
    "            'cls_bboxes': torch.tensor(labels) if labels else torch.zeros((0, 5)),\n",
    "            'ori_shape': ori_shape,\n",
    "            'ratio_pad': ratio_pad,\n",
    "            'im_file': self.data[idx]['url'] # Required for tracking\n",
    "        }\n",
    "\n",
    "# 2. Custom Collate to handle variable numbers of objects per image\n",
    "def custom_collate_fn(batch):\n",
    "    imgs = []\n",
    "    all_bboxes = []\n",
    "    all_cls = []\n",
    "    all_batch_idx = []\n",
    "    \n",
    "    # New metadata lists\n",
    "    ori_shapes = []\n",
    "    ratio_pads = []\n",
    "    im_files = []\n",
    "\n",
    "    for i, item in enumerate(batch):\n",
    "        imgs.append(item['img'])\n",
    "        ori_shapes.append(item['ori_shape'])\n",
    "        ratio_pads.append(item['ratio_pad'])\n",
    "        im_files.append(item['im_file'])\n",
    "        \n",
    "        labels = item['cls_bboxes']\n",
    "        num_objs = labels.shape[0]\n",
    "        if num_objs > 0:\n",
    "            all_batch_idx.append(torch.full((num_objs,), i))\n",
    "            all_cls.append(labels[:, 0])\n",
    "            all_bboxes.append(labels[:, 1:])\n",
    "\n",
    "    stacked_imgs = torch.stack(imgs, 0)\n",
    "\n",
    "    output = {\n",
    "        'img': stacked_imgs,\n",
    "        'batch_idx': torch.cat(all_batch_idx, 0) if all_batch_idx else torch.zeros(0),\n",
    "        'cls': torch.cat(all_cls, 0).view(-1, 1) if all_cls else torch.zeros((0, 1)),\n",
    "        'bboxes': torch.cat(all_bboxes, 0) if all_bboxes else torch.zeros((0, 4)),\n",
    "        # METADATA FOR VALIDATOR:\n",
    "        'ori_shape': ori_shapes,\n",
    "        'ratio_pad': ratio_pads,\n",
    "        'im_file': im_files\n",
    "    }\n",
    "    return output\n",
    "\n",
    "# 3. Custom Trainer to override the data loading logic\n",
    "class URLStreamTrainer(DetectionTrainer):\n",
    "    # We define these as class-level placeholders\n",
    "    train_data_list = []\n",
    "    val_data_list = []\n",
    "\n",
    "    def build_dataset(self, img_path, mode=\"train\", batch=None):\n",
    "        \"\"\"\n",
    "        Overrides the dataset builder. \n",
    "        'mode' tells us if we are loading the 'train' or 'val' set.\n",
    "        \"\"\"\n",
    "        data_source = self.train_data_list if mode == \"train\" else self.val_data_list\n",
    "        \n",
    "        return YOLOStreamingDataset(\n",
    "            data_source, \n",
    "            imgsz=self.args.imgsz\n",
    "        )\n",
    "\n",
    "    def get_dataloader(self, dataset_path, batch_size=16, rank=0, mode=\"train\"):\n",
    "        dataset = self.build_dataset(dataset_path, mode=mode)\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=(mode == \"train\"),\n",
    "            num_workers=self.args.workers,\n",
    "            collate_fn=custom_collate_fn, # Ensure this is defined globally\n",
    "            pin_memory=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54296344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propose dataset format\n",
    "\n",
    "# my__data = [\n",
    "#     {\n",
    "#         'url': 'https://example.com/street_scene.jpg',    # URL to the image\n",
    "#         'bboxes': [[100, 200, 50, 80]],                   # List of bounding boxes in COCO format [x, y, width, height]\n",
    "#         'classes': [1]                                    # Corresponding class IDs for each bounding box\n",
    "#     }\n",
    "#     ...\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbc448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../final_dataset.json\", 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "dataset_labels = [item['classes'][0] for item in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd8661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2, stratify=dataset_labels, random_state=42)\n",
    "\n",
    "URLStreamTrainer.train_data_list = train_data\n",
    "URLStreamTrainer.val_data_list = val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b140334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize YOLO26 model\n",
    "model = YOLO(\"yolo26n.pt\")\n",
    "\n",
    "# Start training using the Custom Trainer\n",
    "model.train(\n",
    "    trainer=URLStreamTrainer,\n",
    "    epochs=5,\n",
    "    imgsz=640,\n",
    "    batch=64,\n",
    "    workers=4,  # Use more workers to mitigate download latency\n",
    "    data=\"wcs_dataset.yaml\", # Needs a dummy yaml just for class names\n",
    "    plots=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88807bce",
   "metadata": {},
   "source": [
    "# Visualize a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c402ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def test_trainer_visualize(trainer, mode=\"train\", max_images=8):\n",
    "    \"\"\"\n",
    "    Hooks into the trainer, pulls one batch, and displays the ground truth in notebook output.\n",
    "    \"\"\"\n",
    "    # 1. Get the dataloader from the trainer\n",
    "    # We use a dummy path because our URLStreamTrainer ignores it\n",
    "    loader = trainer.get_dataloader(dataset_path=\"dummy\", batch_size=16, mode=mode)\n",
    "    \n",
    "    # 2. Grab the first batch\n",
    "    batch = next(iter(loader))\n",
    "    \n",
    "    # 3. Unpack batch\n",
    "    imgs = batch['img']          # [B, 3, 640, 640]\n",
    "    batch_idx = batch['batch_idx'] # [N]\n",
    "    bboxes = batch['bboxes']     # [N, 4] (cx, cy, w, h) normalized\n",
    "    cls = batch['cls']           # [N, 1]\n",
    "    \n",
    "    # Limit the number of images to display\n",
    "    num_images = min(imgs.shape[0], max_images)\n",
    "    \n",
    "    # Calculate subplot grid\n",
    "    cols = min(2, num_images)\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 7.5 * rows))\n",
    "    if num_images == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = [axes] if cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Convert tensor back to numpy image (RGB)\n",
    "        img = imgs[i].permute(1, 2, 0).cpu().numpy() # [640, 640, 3]\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "        \n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Find all objects belonging to this image index\n",
    "        obj_indices = (batch_idx == i).nonzero(as_tuple=True)[0]\n",
    "        \n",
    "        # Display the image\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Batch Item {i} - {mode} ({len(obj_indices)} objects)\")\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Draw bounding boxes\n",
    "        for obj_idx in obj_indices:\n",
    "            # Denormalize coordinates\n",
    "            cx, cy, bw, bh = bboxes[obj_idx]\n",
    "            class_id = int(cls[obj_idx])\n",
    "            \n",
    "            # Convert center-xywh to corners (x1, y1, width, height) for matplotlib\n",
    "            x1 = (cx - bw/2) * w\n",
    "            y1 = (cy - bh/2) * h\n",
    "            bbox_width = bw * w\n",
    "            bbox_height = bh * h\n",
    "            \n",
    "            # Create rectangle patch\n",
    "            rect = patches.Rectangle((x1, y1), bbox_width, bbox_height, \n",
    "                                   linewidth=2, edgecolor='lime', facecolor='none')\n",
    "            axes[i].add_patch(rect)\n",
    "            \n",
    "            # Add class label\n",
    "            axes[i].text(x1, y1 - 5, f\"Class: {class_id}\", \n",
    "                        color='lime', fontsize=10, weight='bold',\n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='black', alpha=0.7))\n",
    "        \n",
    "        print(f\"Image {i}: {len(obj_indices)} objects detected\")\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(num_images, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- HOW TO RUN ---\n",
    "# Assuming you've already initialized your trainer:\n",
    "test_trainer_visualize(URLStreamTrainer(overrides={'model': 'yolo26n.pt', 'data': 'wcs_dataset.yaml'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e0a87e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
